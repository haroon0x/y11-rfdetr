# Based on yolo11.yaml but with additional P2/4 detection head
# Use case: VisDrone, UAV imagery, small person detection

# Parameters
nc: 1  # number of classes (will be overridden by data.yaml)
scales:
  # [depth, width, max_channels]
  n: [0.50, 0.25, 1024]  # yolo11n-p2
  s: [0.50, 0.50, 1024]  # yolo11s-p2
  m: [0.50, 1.00, 512]   # yolo11m-p2
  l: [1.00, 1.00, 512]   # yolo11l-p2
  x: [1.00, 1.50, 512]   # yolo11x-p2

# YOLO11 backbone (unchanged - P2 already exists at layer 2)
backbone:
  # [from, repeats, module, args]
  - [-1, 1, Conv, [64, 3, 2]]          # 0-P1/2
  - [-1, 1, Conv, [128, 3, 2]]         # 1-P2/4
  - [-1, 2, C3k2, [256, False, 0.25]]  # 2 - P2 feature output
  - [-1, 1, Conv, [256, 3, 2]]         # 3-P3/8
  - [-1, 2, C3k2, [512, False, 0.25]]  # 4 - P3 feature output
  - [-1, 1, Conv, [512, 3, 2]]         # 5-P4/16
  - [-1, 2, C3k2, [512, True]]         # 6 - P4 feature output
  - [-1, 1, Conv, [1024, 3, 2]]        # 7-P5/32
  - [-1, 2, C3k2, [1024, True]]        # 8
  - [-1, 1, SPPF, [1024, 5]]           # 9
  - [-1, 2, C2PSA, [1024]]             # 10 - P5 feature output

# YOLO11 head with P2 detection layer
head:
  # === Top-down pathway (FPN) ===
  # P5 -> P4
  - [-1, 1, nn.Upsample, [None, 2, "nearest"]]
  - [[-1, 6], 1, Concat, [1]]          # cat backbone P4
  - [-1, 2, C3k2, [512, False]]        # 13

  # P4 -> P3
  - [-1, 1, nn.Upsample, [None, 2, "nearest"]]
  - [[-1, 4], 1, Concat, [1]]          # cat backbone P3
  - [-1, 2, C3k2, [256, False]]        # 16 (P3/8-small)

  # P3 -> P2 (NEW: extend FPN to P2)
  - [-1, 1, nn.Upsample, [None, 2, "nearest"]]
  - [[-1, 2], 1, Concat, [1]]          # cat backbone P2 (layer 2)
  - [-1, 2, C3k2, [128, False]]        # 19 (P2/4-tiny) <- NEW P2 detection feature

  # === Bottom-up pathway (PAN) ===
  # P2 -> P3
  - [-1, 1, Conv, [128, 3, 2]]
  - [[-1, 16], 1, Concat, [1]]         # cat head P3
  - [-1, 2, C3k2, [256, False]]        # 22 (P3/8-small)

  # P3 -> P4
  - [-1, 1, Conv, [256, 3, 2]]
  - [[-1, 13], 1, Concat, [1]]         # cat head P4
  - [-1, 2, C3k2, [512, False]]        # 25 (P4/16-medium)

  # P4 -> P5
  - [-1, 1, Conv, [512, 3, 2]]
  - [[-1, 10], 1, Concat, [1]]         # cat head P5
  - [-1, 2, C3k2, [1024, True]]        # 28 (P5/32-large)

  # === Detection heads (4 scales: P2, P3, P4, P5) ===
  - [[19, 22, 25, 28], 1, Detect, [nc]]  # Detect(P2, P3, P4, P5)

# Notes:
# - P2/4 stride: detects objects as small as ~4 pixels
# - P3/8 stride: detects small objects (~8-16 pixels)
# - P4/16 stride: detects medium objects (~16-32 pixels)  
# - P5/32 stride: detects large objects (~32+ pixels)
#
# Expected impact:
# - ~1.5-2x more GFLOPs than standard yolo11n
# - ~1.5x more parameters
# - Significantly better mAP for small objects (especially in drone/UAV imagery)
# - May need to reduce batch size due to higher memory usage
#
# Usage:
#   from ultralytics import YOLO
#   model = YOLO("configs/yolo11n-p2.yaml")
#   model.train(data="data/visdrone_person/data.yaml", imgsz=1280, ...)
