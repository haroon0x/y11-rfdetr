{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Long-Range Person Detection - Advanced (with SAHI)\n",
                "\n",
                "This notebook runs the YOLOv11 training pipeline AND installs SAHI for enhanced small object detection.\n",
                "It uses `pip` to install dependencies directly into the Colab environment."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# 1. Clone the Repository\n",
                "!git clone https://github.com/haroon0x/y11-rfdetr.git\n",
                "%cd y11-rfdetr"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# 2. Install Dependencies\n",
                "# We install standard requirements plus SAHI explicitly\n",
                "!pip install -r requirements.txt\n",
                "!pip install ultralytics\n",
                "!pip install sahi  # <--- Installing SAHI explicitly as requested"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# 3. Verify Environment\n",
                "import torch\n",
                "print(f\"CUDA Available: {torch.cuda.is_available()}\")\n",
                "if torch.cuda.is_available():\n",
                "    print(f\"Device: {torch.cuda.get_device_name(0)}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# 4. Run Training Pipeline\n",
                "# Defaulting to Nano (yolo11n.pt) as requested.\n",
                "!python scripts/train_pipeline.py --model yolo11n.pt"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# 5. Display Training Results (Graphs)\n",
                "import os\n",
                "from IPython.display import Image, display\n",
                "\n",
                "# Find the latest training run directory\n",
                "base_dir = \"yolo11m_training_runs\"\n",
                "if os.path.exists(base_dir):\n",
                "    steps = sorted(os.listdir(base_dir))\n",
                "    if steps:\n",
                "        latest_step = steps[-1]\n",
                "        results_path = os.path.join(base_dir, latest_step, \"results.png\")\n",
                "        confusion_matrix_path = os.path.join(base_dir, latest_step, \"confusion_matrix.png\")\n",
                "        \n",
                "        print(f\"Displaying results for: {latest_step}\")\n",
                "        if os.path.exists(results_path):\n",
                "            display(Image(filename=results_path))\n",
                "        else:\n",
                "            print(\"results.png not found.\")\n",
                "            \n",
                "        if os.path.exists(confusion_matrix_path):\n",
                "            display(Image(filename=confusion_matrix_path))\n",
                "    else:\n",
                "        print(\"No training steps found.\")\n",
                "else:\n",
                "    print(\"Training output directory not found.\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# 6. Run Benchmark\n",
                "# Benchmarks the best model from the first step (VisDrone) on the validation set\n",
                "!yolo benchmark model=yolo11m_training_runs/step1_visdrone/weights/best.pt data=VisDrone.yaml imgsz=640 half=True device=0"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# 7. Run SAHI Inference (Example)\n",
                "# This runs the SAHI script on a sample image from the test set\n",
                "# Replace 'path/to/image.jpg' with an actual image path from your dataset\n",
                "# !python scripts/inference_sahi.py --model yolo11m_training_runs/step1_visdrone/weights/best.pt --source data/visdrone/VisDrone2019-DET-test-dev/images/0000001_02999_d_0000005.jpg"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# 8. Download Results\n",
                "!zip -r runs_sahi.zip yolo11m_training_runs/ runs/sahi_inference/\n",
                "from google.colab import files\n",
                "files.download('runs_sahi.zip')"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.8.5"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}