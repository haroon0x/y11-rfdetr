{
    "cells": [
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "vscode": {
                    "languageId": "plaintext"
                }
            },
            "outputs": [],
            "source": [
                "# Long-Range Person Detection - Standard Training (YOLOv11)\n",
                "\n",
                "This notebook runs the standard YOLOv11 training pipeline without SAHI.\n",
                "It uses `pip` to install dependencies directly into the Colab environment."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# 1. Clone the Repository\n",
                "!git clone https://github.com/haroon0x/y11-rfdetr.git\n",
                "%cd y11-rfdetr"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# 2. Install Dependencies\n",
                "# In Colab, we use pip directly. We don't need 'uv' virtual environments here.\n",
                "!pip install -r requirements.txt\n",
                "!pip install ultralytics"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# 3. Verify Environment\n",
                "import torch\n",
                "print(f\"CUDA Available: {torch.cuda.is_available()}\")\n",
                "if torch.cuda.is_available():\n",
                "    print(f\"Device: {torch.cuda.get_device_name(0)}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# 4. Run Training Pipeline\n",
                "# Using optimized settings for Colab T4 GPU to avoid OOM\n",
                "!python scripts/train_pipeline.py --epochs 50 --batch 8 --imgsz 960 --workers 2 --model yolo11m.pt"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# 5. Display Training Results (Graphs)\n",
                "import os\n",
                "from IPython.display import Image, display\n",
                "\n",
                "# Find the latest training run directory\n",
                "base_dir = \"yolo11m_training_runs\"\n",
                "if os.path.exists(base_dir):\n",
                "    steps = sorted(os.listdir(base_dir))\n",
                "    if steps:\n",
                "        latest_step = steps[-1]\n",
                "        results_path = os.path.join(base_dir, latest_step, \"results.png\")\n",
                "        confusion_matrix_path = os.path.join(base_dir, latest_step, \"confusion_matrix.png\")\n",
                "        \n",
                "        print(f\"Displaying results for: {latest_step}\")\n",
                "        if os.path.exists(results_path):\n",
                "            display(Image(filename=results_path))\n",
                "        else:\n",
                "            print(\"results.png not found.\")\n",
                "            \n",
                "        if os.path.exists(confusion_matrix_path):\n",
                "            display(Image(filename=confusion_matrix_path))\n",
                "    else:\n",
                "        print(\"No training steps found.\")\n",
                "else:\n",
                "    print(\"Training output directory not found.\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# 6. Run Benchmark\n",
                "# Benchmarks the best model from the first step (VisDrone) on the validation set\n",
                "!yolo benchmark model=yolo11m_training_runs/step1_visdrone/weights/best.pt data=VisDrone.yaml imgsz=640 half=True device=0"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# 7. Download Results\n",
                "!zip -r runs_standard.zip yolo11m_training_runs/\n",
                "from google.colab import files\n",
                "files.download('runs_standard.zip')"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.8.5"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}
